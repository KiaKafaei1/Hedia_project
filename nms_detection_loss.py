# -*- coding: utf-8 -*-
"""nms_detection_loss.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-pBT59PnYE4Y9oCpEgZh8t6kuxU4zK-H
"""

import matplotlib
import numpy as np
import pandas as pd
import torch 
import torchvision
import torch.nn as nn
import torch.nn.functional as F 
from torchvision import transforms
import cv2 
from PIL import Image
import matplotlib.pyplot as plt
import glob
import os
from IPython.display import clear_output
from skimage.io import imread
from skimage.transform import resize
from google.colab import drive

# run GPU .... 
if (torch.cuda.is_available()):
    device = torch.device("cuda")
    print(device, torch.cuda.get_device_name(0))
else:
    device= torch.device("cpu")
    print(device)

def nms(anchors, pred_anchor_locs, objectness_score, nms_thresh = 0.7, n_train_pre_nms = 12000, n_train_post_nms = 2000, min_size = 16):

    ############################################################################
    # This function nms (non-maximum supression)
    # inputs:
    #   anchors
    #   pred_anchor_locs
    #   objectness_score

    # outputs:
    #   roi (region of interest) contains a matrix of size 4*2000 (x_min, y_min, x_max, y_max) which is the coordinate of 2000 rois

    ############################################################################

    anc_height = anchors[:, 2] - anchors[:, 0]
    anc_width = anchors[:, 3] - anchors[:, 1]
    anc_ctr_y = anchors[:, 0] + 0.5 * anc_height
    anc_ctr_x = anchors[:, 1] + 0.5 * anc_width

    # The 22500 anchor boxes location and labels predicted by RPN (convert to numpy)
    # format = (dy, dx, dh, dw)
    pred_anchor_locs_numpy = pred_anchor_locs[0].cpu().data.numpy()
    objectness_score_numpy = objectness_score[0].cpu().data.numpy()

    # Normalize
    # pred_anchor_locs_numpy = (pred_anchor_locs_numpy - np.mean(pred_anchor_locs_numpy, axis=0)) / np.std(pred_anchor_locs_numpy, axis=0)

    dy = pred_anchor_locs_numpy[:, 0::4] # anchor box dy
    dx = pred_anchor_locs_numpy[:, 1::4] # dx
    dh = pred_anchor_locs_numpy[:, 2::4] # dh
    dw = pred_anchor_locs_numpy[:, 3::4] # dw

    # ctr_y = dy predicted by RPN * anchor_h + anchor_cy
    # ctr_x similar
    # h = exp(dh predicted by RPN) * anchor_h
    # w similar
    ctr_y = dy * anc_height[:, np.newaxis] + anc_ctr_y[:, np.newaxis]
    ctr_x = dx * anc_width[:, np.newaxis] + anc_ctr_x[:, np.newaxis]
    h = np.exp(dh) * anc_height[:, np.newaxis]
    w = np.exp(dw) * anc_width[:, np.newaxis]
    #print(w.shape)

    # ROI sampling:
    roi = np.zeros(pred_anchor_locs_numpy.shape)
    roi[:, 0::4] = ctr_y - 0.5 * h
    roi[:, 1::4] = ctr_x - 0.5 * w
    roi[:, 2::4] = ctr_y + 0.5 * h
    roi[:, 3::4] = ctr_x + 0.5 * w
    #print(roi.shape)

    # clip the predicted boxes to the image
    img_size = (800, 800) #Image size
    roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0])
    roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1])
    # print(roi.shape, np.max(roi), np.min(roi))

    # Remove predicted boxes with either height or width < threshold.
    hs = roi[:, 2] - roi[:, 0]
    ws = roi[:, 3] - roi[:, 1]
    keep = np.where((hs >= min_size) & (ws >= min_size))[0] #min_size=16
    roi = roi[keep, :]
    score = objectness_score_numpy[keep]
    #print(keep.shape, roi.shape, score.shape)

    # Sort all (proposal, score) pairs by score from highest to lowest
    order = score.ravel().argsort()[::-1]
    #print(order.shape)

    #Take top pre_nms_topN (e.g. 12000 while training and 300 while testing)
    order = order[:n_train_pre_nms]
    roi = roi[order, :]
    #print(order.shape, roi.shape, roi.shape)
    # Take all the roi boxes [roi_array]
    y1 = roi[:, 0]
    x1 = roi[:, 1]
    y2 = roi[:, 2]
    x2 = roi[:, 3]

    # Find the areas of all the boxes [roi_area]
    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    #Take the indexes of order the probability score in descending order
    order = order.argsort()[::-1]
    keep = []
    while (order.size > 0):
        i = order[0] #take the 1st elt in order and append to keep
        keep.append(i)
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])
        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        inter = w * h
        ovr = inter / (areas[i] + areas[order[1:]] - inter)
        inds = np.where(ovr <= nms_thresh)[0]
        order = order[inds + 1]
    keep = keep[:n_train_post_nms] # while training/testing , use accordingly
    roi = roi[keep] # the final region proposals
    # print(len(keep), roi.shape)
    return roi

def sampleRoi(roi,bbox,labels, n_sample = 128, pos_ratio = 0.25, pos_iou_thresh = 0.5, neg_iou_thresh_hi = 0.5, neg_iou_thresh_lo = 0.0):
    #NMS 2000 ROIs, gt box iou, 128 ROI samples, positive 128x0.25=32.

    # inputs:
    #   roi from NMS (non-maximum supression)
    #   bbox ground truth bounding box
    #   labels
    # outputs:
    #   pos_index, neg_index (index of positive labels can be used for visualization)
    #                         pos_index is object, neg_index is background
    #   gt_roi_locs ground truth roi locations in format - > (dy, dx, dh, dw)
    #   gt_roi_labels


    n_sample = n_sample  # Number of samples from roi
    pos_ratio = pos_ratio # Number of positive examples out of the n_samples
    pos_iou_thresh = pos_iou_thresh  # Min iou of region proposal with any groundtruth object to consider it as positive label
    neg_iou_thresh_hi = neg_iou_thresh_hi  # iou 0~0.5 is considered as negative (0, background)
    neg_iou_thresh_lo = neg_iou_thresh_lo

    # Find the iou of each ground truth object with the region proposals,
    ious = np.empty((len(roi), len(bbox[:,1])), dtype=np.float32)
    ious.fill(0)
    for num1, i in enumerate(roi):
        ya1, xa1, ya2, xa2 = i
        anchor_area = (ya2 - ya1) * (xa2 - xa1)
        for num2, j in enumerate(bbox):
            yb1, xb1, yb2, xb2 = j
            box_area = (yb2- yb1) * (xb2 - xb1)
            inter_x1 = max([xb1, xa1])
            inter_y1 = max([yb1, ya1])
            inter_x2 = min([xb2, xa2])
            inter_y2 = min([yb2, ya2])
            if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):
                iter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)
                iou = iter_area / (anchor_area+ box_area - iter_area)
            else:
                iou = 0.
            ious[num1, num2] = iou #(intersection over union)

    #print(ious.shape)
    # Find out which ground truth has high IoU for each region proposal, Also find the maximum IoU
    gt_assignment = ious.argmax(axis=1)
    max_iou = ious.max(axis=1)
    #print(gt_assignment)
    #print(max_iou)

    # Assign the labels to each proposal
    gt_roi_label = labels[gt_assignment]
    #print(gt_roi_label)
    # Select the foreground rois as per the pos_iou_thesh and
    # n_sample x pos_ratio (128 x 0.25 = 32) foreground samples.
    pos_roi_per_image = n_sample*pos_ratio
    pos_index = np.where(max_iou >= pos_iou_thresh)[0]
    # # Allow threshold to be lowered if only background RoI
    # n = 0
    # while pos_index.size==0:
    #   pos_iou_thresh = pos_iou_thresh*0.95
    #   neg_iou_thresh_hi = 1-pos_iou_thresh
    #   pos_index = np.where(max_iou >= pos_iou_thresh)[0]
    #   n = n + 1
    #   if n==200:
    #     print("Too low positive threshold -> NaN value")
    #     break
    # if n!=0:
    #   print("IoU threshold lowered: ", n)
      
    pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))
    if pos_index.size > 0:
        pos_index = np.random.choice(
            pos_index, size=pos_roi_per_this_image, replace=False)
    #print(pos_roi_per_this_image)
    #print(pos_index)

    # Similarly we do for negitive (background) region proposals
    neg_index = np.where((max_iou < neg_iou_thresh_hi) &
                                 (max_iou >= neg_iou_thresh_lo))[0]
    neg_roi_per_this_image = n_sample - pos_roi_per_this_image
    neg_roi_per_this_image = int(min(neg_roi_per_this_image, neg_index.size))
    if  neg_index.size > 0 :
        neg_index = np.random.choice(
            neg_index, size=neg_roi_per_this_image, replace=False)
    #print(neg_roi_per_this_image)
    #print(neg_index)

    # Now we gather positve samples index and negitive samples index,
    # their respective labels and region proposals

    keep_index = np.append(pos_index, neg_index)
    gt_roi_labels = gt_roi_label[keep_index]
    gt_roi_labels[pos_roi_per_this_image:] = 0  # negative labels --> 0
    sample_roi = roi[keep_index]
    #print(sample_roi)

    # Pick the ground truth objects for these sample_roi and
    # later parameterize as we have done while assigning locations to anchor boxes in section 2.
    bbox_for_sampled_roi = bbox[gt_assignment[keep_index]]
    #print(bbox_for_sampled_roi.shape)

    height = sample_roi[:, 2] - sample_roi[:, 0]
    width = sample_roi[:, 3] - sample_roi[:, 1]
    ctr_y = sample_roi[:, 0] + 0.5 * height
    ctr_x = sample_roi[:, 1] + 0.5 * width

    base_height = bbox_for_sampled_roi[:, 2] - bbox_for_sampled_roi[:, 0]
    base_width = bbox_for_sampled_roi[:, 3] - bbox_for_sampled_roi[:, 1]
    base_ctr_y = bbox_for_sampled_roi[:, 0] + 0.5 * base_height
    base_ctr_x = bbox_for_sampled_roi[:, 1] + 0.5 * base_width

    eps = np.finfo(height.dtype).eps
    height = np.maximum(height, eps)
    width = np.maximum(width, eps)

    dy = (base_ctr_y - ctr_y) / height
    dx = (base_ctr_x - ctr_x) / width
    dh = np.log(base_height / height)
    dw = np.log(base_width / width)

    gt_roi_locs = np.vstack((dy, dx, dh, dw)).transpose()

    # Normalize gt
    gt_roi_locs = (gt_roi_locs - np.mean(gt_roi_locs, axis=0)) / np.std(gt_roi_locs, axis=0)
    #changed format of the locations

    return pos_index, neg_index, gt_roi_locs, gt_roi_labels, sample_roi, pos_roi_per_this_image

def ROIPooling(sample_roi, out_map):
    # ROI poolinng is the same as maxpooling just performed on the regions of interest.
    # The value it chooses is the largest value from each feature map in the region of interest.
    # This is the output we insert in the Detection Network.
    # We use maxpooling to make the data smaller and standardized.

    # input:
    #   sample_roi
    # outputs:
    #   rois
    #   out_maxPool

    # parameters:
    # kernel size for max poolinng:
    size = (7, 7)

    rois = torch.from_numpy(sample_roi).float()
    roi_indices = 0 * np.ones((len(rois),), dtype=np.int32)
    roi_indices = torch.from_numpy(roi_indices).float()
    #print(rois.shape, roi_indices.shape)

    indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)
    xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]
    indices_and_rois = xy_indices_and_rois.contiguous()
    #print(xy_indices_and_rois.shape)
    adaptive_max_pool = nn.AdaptiveMaxPool2d(size[0], size[1])
    output = []
    rois = indices_and_rois.data.float()
    rois[:, 1:].mul_(1/16.0) # Subsampling ratio
    rois = rois.long()
    num_rois = rois.size(0)
    for i in range(num_rois):
        roi = rois[i]
        im_idx = roi[0]
        im = out_map.narrow(0, im_idx, 1)[..., roi[2]:(roi[4]+1), roi[1]:(roi[3]+1)]
        tmp = adaptive_max_pool(im)
        output.append(tmp[0])
    output = torch.cat(output, 0)
    #print(output.size())
    # Reshape the tensor so that we can pass it through the feed forward layer.
    # This is the output after pooling
    out_maxPool = output.view(output.size(0), -1)
    #print(k.shape) # 25088 = 7*7*512

    return rois, out_maxPool

def detectionNetwork(out_maxPool, gt_roi_locs, gt_roi_labels):
    roi_lambda = 10.
    # 7x7x512 = 25088 this is the input size and 4096 is the output size which could be an image of 64x64.
    # we first have a layer that reduces the input and the second layer doesn't reduce the input.
    # The third layer reduces the image to an ouput list of size 8. Consiting of 2 classes (forground and background) and 4 coordinates which
    # is the coordinate and a height and width.

    # We then initialize the weight with a normal distribution with mean =0 and std = 0.01
    # The bias is intialized to 0

    roi_head_classifier = nn.Sequential(*[nn.Linear(25088, 4096), nn.Linear(4096, 4096)]).to(device)
    cls_loc = nn.Linear(4096, 11 * 4).to(device) # (11 classes + 1 background. Each will have 4 co-ordinates)
    # just in case....!!!!!
    # cls_loc = nn.Linear(4096, 11 * 4).to(device).cuda()
    cls_loc.weight.data.normal_(0, 0.01)
    cls_loc.bias.data.zero_()

    score = nn.Linear(4096, 11).to(device) # (11 classes, + 1 background)
    # passing the output of roi-pooling to ROI head
    # We take the output from the pooling and insert it in the detection network

    # The way it works overall is that a set of regions are detected for an image,
    # corresponding to each object in the image. For each object the region associated with it
    # is transformed to an image of size 64x64 and this image is then the input to the
    # detection network. The detection network then outputs the location of the object and background.
    # And it outputs the score i.e the probability that the classes are true. I.e 0.86 means
    # the network is 86 % sure that the region is an object.
    # 128 is the number of regions.


    k = roi_head_classifier(out_maxPool.to(device))
    # It classifies the location and the score of the location
    roi_cls_loc = cls_loc(k)
    roi_cls_score = score(k)
    #print(roi_cls_loc.shape, roi_cls_score.shape)
    gt_roi_labels=gt_roi_labels.flatten()
    # Converting ground truth to torch variable
    gt_roi_loc = torch.from_numpy(gt_roi_locs)
    gt_roi_label = torch.from_numpy(np.float32(gt_roi_labels)).long()
    #print(gt_roi_loc.shape, gt_roi_label.shape)
    #print(gt_roi_loc)

    #Classification loss
    roi_cls_loss = F.cross_entropy(roi_cls_score.cpu(), gt_roi_label.cpu(), ignore_index=-1)
    #print(roi_cls_loss.shape)
    #print(roi_cls_loss)
    # Regression loss
    n_sample = roi_cls_loc.shape[0]
    roi_loc = roi_cls_loc.view(n_sample, -1, 4)
    #print(roi_loc.shape)

    roi_loc = roi_loc[torch.arange(0, n_sample).long(), gt_roi_label]
    #print(roi_loc.shape)

    # For Regression we use smooth L1 loss as defined in the Fast RCNN paper
    pos = gt_roi_label > 0
    mask = pos.unsqueeze(1).expand_as(roi_loc)
    #print(mask.shape)

    # take those bounding boxes which have positve labels
    mask_loc_preds = roi_loc[mask].view(-1, 4)
    mask_loc_targets = gt_roi_loc[mask].view(-1, 4)
    #print(mask_loc_preds.shape, mask_loc_targets.shape)
    #print(mask_loc_targets)
    #print(mask_loc_preds)

    x = torch.abs(mask_loc_targets.cpu() - mask_loc_preds.cpu())
    roi_loc_loss = ((x < 1).float() * 0.5 * x**2) + ((x >= 1).float() * (x-0.5))
    #roi_loc_loss.sum() is the total regression loss
    #print(roi_loc_loss.sum())

    #roi_lambda is a hyperparameter

    ##not sure which score
    N_reg = (gt_roi_labels >0).sum()
    roi_loc_loss = roi_loc_loss.sum() / N_reg
    roi_loss = roi_cls_loss + (roi_lambda * roi_loc_loss)

    return roi_loss